# Scholarly Comparison Configuration: Traditional RL vs. Shaping-Based RL
# Research Objective: Compare learning efficiency between standard RL and dog training-inspired shaping methods

# Experimental Design
experiment:
  name: "Traditional_RL_vs_Shaping_RL_Comparison"
  hypothesis: "Shaping-based RL will achieve faster learning and better final performance than traditional RL"
  independent_variable: "Training Method"  # Traditional RL vs Shaping RL
  dependent_variables:
    - "Learning Speed (episodes to first success)"
    - "Final Performance (score, completion time)"
    - "Learning Stability (reward variance)"
    - "Behavioral Complexity (action diversity)"
  control_variables:
    - "Network Architecture"
    - "Learning Algorithm (PPO)"
    - "Environment (Sonic 1 Green Hill Zone)"
    - "Training Duration"
    - "Random Seed"

# Game Settings (Identical for both methods)
game:
  name: "sonic1"
  rom_path: "roms/sonic1.md"
  frame_skip: 4
  max_steps: 12000
  render: false
  objective: "Complete Green Hill Zone Act 3"

# Environment Settings (Identical for both methods)
environment:
  screen_width: 224
  screen_height: 256
  grayscale: true
  stack_frames: 4
  normalize_obs: true
  reward_scale: 1.0

# Agent Settings (Identical for both methods)
agent:
  type: "ppo"
  learning_rate: 0.00025
  batch_size: 128
  buffer_size: 50000
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  n_steps: 2048

# Network Architecture (Identical for both methods)
network:
  type: "mlp"
  mlp_layers: [512, 256, 128]
  activation: "relu"

# Training Settings (Identical for both methods)
training:
  total_timesteps: 2000000
  save_interval: 50000
  eval_interval: 25000
  log_interval: 100
  checkpoint_dir: "models/scholarly_comparison/"
  log_dir: "logs/scholarly_comparison/"
  
# Method A: Traditional RL Rewards (Control Group)
traditional_rl_rewards:
  # Sparse, outcome-based rewards (standard RL approach)
  level_completed: 3000.0  # Large reward for final objective
  ring_collected: 15.0     # Direct outcome reward
  enemy_defeated: 8.0      # Direct outcome reward
  power_up_collected: 20.0 # Direct outcome reward
  checkpoint_reached: 100.0 # Direct outcome reward
  
  # Penalties for failures
  game_over: -200.0        # Severe penalty for complete failure
  fall_penalty: -15.0      # Penalty for falling
  stuck_penalty: -2.0      # Penalty for being stuck
  
  # Minimal guidance rewards
  forward_progress: 1.0    # Small reward for moving right
  time_penalty: -0.1       # Small time pressure

# Method B: Shaping-Based RL Rewards (Experimental Group)
shaping_rl_rewards:
  # Progressive behavior shaping (dog training inspired)
  
  # Phase 1: Micro-level behaviors (basic movements)
  micro_rewards:
    move_right: 0.5        # Reward for right movement
    jump_action: 1.0       # Reward for jumping
    exploration: 0.5       # Reward for visiting new areas
  
  # Phase 2: Mid-level behaviors (skill combinations)
  mid_rewards:
    successful_jump: 0.5   # Reward for successful jumps
    speed_maintenance: 1.0 # Reward for maintaining speed
    ring_collection: 2.0   # Reward for collecting rings
  
  # Phase 3: Macro-level behaviors (strategic actions)
  macro_rewards:
    obstacle_avoidance: 3.0    # Reward for avoiding obstacles
    enemy_defeat: 5.0          # Reward for defeating enemies
    power_up_usage: 4.0        # Reward for using power-ups
  
  # Phase 4: Termination-level (final objectives)
  termination_rewards:
    level_completed: 1500.0    # Large reward for completion
    checkpoint_reached: 50.0   # Intermediate goal reward
  
  # Shaping-specific features
  shaping_features:
    velocity_bonus: 0.001      # Reward for movement speed
    invincibility_bonus: 0.5   # Reward for power-up states
    shield_bonus: 0.2          # Reward for defensive states
    time_efficiency: -0.001    # Time pressure for efficiency

# Observation Processing (Identical for both methods)
observation:
  resize: [84, 84]
  crop: [0, 0, 224, 256]
  normalize: true
  frame_stack: 4

# Action Space (Identical for both methods)
actions:
  basic:
    - "NOOP"
    - "LEFT"
    - "RIGHT"
    - "UP"
    - "DOWN"
    - "A"  # Jump
    - "B"  # Spin dash
    - "START"
    - "SELECT"
  combinations:
    - ["LEFT", "A"]  # Jump left
    - ["RIGHT", "A"]  # Jump right
    - ["DOWN", "B"]  # Spin dash
    - ["LEFT", "DOWN", "B"]  # Spin dash left
    - ["RIGHT", "DOWN", "B"]  # Spin dash right

# Hardware Settings (Identical for both methods)
hardware:
  device: "auto"
  num_envs: 4
  num_threads: 8
  gpu_memory_fraction: 0.8

# Evaluation Settings (Identical for both methods)
evaluation:
  eval_freq: 25000
  n_eval_episodes: 10
  deterministic: true
  render: true

# Scholarly Metrics and Logging
scholarly_metrics:
  # Learning Speed Metrics
  learning_speed:
    episodes_to_first_success: true
    steps_to_first_success: true
    learning_curve_slope: true
  
  # Performance Metrics
  performance:
    final_score: true
    completion_time: true
    success_rate: true
    average_reward_per_episode: true
  
  # Behavioral Metrics
  behavior:
    action_diversity: true
    exploration_coverage: true
    skill_usage_frequency: true
    behavioral_consistency: true
  
  # Stability Metrics
  stability:
    reward_variance: true
    learning_curve_smoothness: true
    convergence_stability: true
  
  # Comparative Metrics
  comparison:
    relative_improvement: true
    statistical_significance: true
    effect_size: true

# Logging Configuration
logging:
  tensorboard: true
  wandb: false
  save_videos: true
  video_fps: 30
  log_episode_rewards: true
  log_episode_lengths: true
  log_episode_scores: true
  log_episode_rings: true
  log_episode_lives: true
  log_training_loss: true
  log_learning_rate: true
  log_behavioral_metrics: true
  log_shaping_phases: true

# Statistical Analysis Settings
statistical_analysis:
  confidence_level: 0.95
  significance_threshold: 0.05
  effect_size_threshold: 0.2
  sample_size: 10  # Number of independent runs per method
  random_seeds: [42, 123, 456, 789, 101112, 131415, 161718, 192021, 222324, 252627]
